# -*- coding: utf-8 -*-
"""autoencode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vn-qzDsHUYJM86F4_tQHl6l21RYAWdb-
"""

import tensorflow as tf

import math
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras import Model
from tensorflow.keras import Sequential
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.layers import Dense, Dropout
from sklearn.model_selection import train_test_split
from tensorflow.keras.losses import MeanSquaredLogarithmicError

train_data = pd.read_csv('X_train.csv')
test_data = pd.read_csv('X_test.csv')

from sklearn.preprocessing import MinMaxScaler

standard_scaler = MinMaxScaler()
x_train_scaled = standard_scaler.fit_transform(train_data)
x_test_scaled = standard_scaler.transform(test_data)

class AutoEncoders(Model):

  def __init__(self, output_units):

    super().__init__()
    print(output_units)
    self.encoder = Sequential(
        [
          Dense(512, activation="relu",kernel_regularizer='l1'),
          Dense(7, activation="relu")
        ]
    )

    self.decoder = Sequential(
        [
          Dense(256, activation="relu"),
          Dense(output_units, activation="sigmoid")
        ]
    )

  def call(self, inputs):

    encoded = self.encoder(inputs)
    decoded = self.decoder(encoded)
    return decoded
  
auto_encoder = AutoEncoders(len(x_train_scaled[0]))

auto_encoder.compile(
    loss='mae',
    metrics=['mae'],
    optimizer='adam'
)

history = auto_encoder.fit(
    x_train_scaled, 
    x_train_scaled, 
    epochs=15, 
    batch_size=32,
    validation_data=(x_test_scaled, x_test_scaled)

)

x_train_scaled[0]

auto_encoder.summary()

reduced_df=auto_encoder.predict(x_train_scaled)

encoder_layer = auto_encoder.get_layer('sequential')
reduced_df = pd.DataFrame(encoder_layer.predict(x_train_scaled))
reduced_df = reduced_df.add_prefix('feature_')

len(reduced_df)

reduced_df

from sklearn.ensemble import IsolationForest
if_model=IsolationForest(n_estimators=100, contamination=0.03)
if_model.fit(reduced_df)
print(if_model)

score_pred = if_model.decision_function(reduced_df)
#print(score_pred)
pred = if_model.predict(reduced_df)
#print(pred)
print(type(pred))

import numpy as np

arr = np.array([pred])
list1 = arr.tolist()
print(list1)
print(type(list1))
from collections import Counter
Counter(pred)

#len(reduced_df[0])

