{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "y = pd.read_csv('novacontent.csv', header=0)\n",
    "#to drop first column\n",
    "y=y.iloc[:, 1:]\n",
    "y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(y['Content'])\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer.inverse_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['Content']=y['Content'].str.lower()\n",
    "y['Content']=y['Content'].apply(word_tokenize)\n",
    "#y.to_csv('content.csv')\n",
    "y.to_csv('tokens.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words=[]\n",
    "for i in y['Content']:\n",
    "    for j in i:\n",
    "        all_words.append(j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without applying autoencoders for feature extraction(here it is words)\n",
    "\n",
    "if_model=IsolationForest(n_estimators=100, contamination=0.03)\n",
    "if_model.fit(X.toarray())\n",
    "print(if_model)\n",
    "score_pred = if_model.decision_function(X.toarray())\n",
    "#print(score_pred)\n",
    "pred = if_model.predict(X.toarray())\n",
    "#print(pred)\n",
    "print(type(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shows that there are 740 positive logs and 21 negative logs\n",
    "\n",
    "arr = np.array([pred])\n",
    "list1 = arr.tolist()\n",
    "print(list1)\n",
    "print(type(list1))\n",
    "Counter(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.toarray()[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the data to randomly pick the rows for training and testing\n",
    "\n",
    "import random\n",
    "random.shuffle(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data to train and test\n",
    "\n",
    "split_len = 0.7 * len(Y)\n",
    "split_len\n",
    "X_train = Y[:int(split_len)]\n",
    "X_test = Y[int(split_len):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_train.to_csv('X_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.DataFrame(X_test)\n",
    "X_test.to_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling the data\n",
    "\n",
    "standard_scaler = MinMaxScaler()\n",
    "x_train_scaled = standard_scaler.fit_transform(X_train)\n",
    "x_test_scaled = standard_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoders(Model):\n",
    "\n",
    "  def __init__(self, output_units):\n",
    "\n",
    "    super().__init__()\n",
    "    self.encoder = Sequential(\n",
    "        [\n",
    "          Dense(32, activation=\"relu\"),\n",
    "          Dense(16, activation=\"relu\"),\n",
    "          Dense(7, activation=\"relu\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    self.decoder = Sequential(\n",
    "        [\n",
    "          Dense(16, activation=\"relu\"),\n",
    "          Dense(32, activation=\"relu\"),\n",
    "          Dense(output_units, activation=\"sigmoid\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def call(self, inputs):\n",
    "\n",
    "  encoded = self.encoder(inputs)\n",
    "  decoded = self.decoder(encoded)\n",
    "  return decoded\n",
    "  \n",
    "auto_encoder = AutoEncoders(len(x_train_scaled))\n",
    "\n",
    "auto_encoder.compile(\n",
    "    loss='mae',\n",
    "    metrics=['mae'],\n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "history = auto_encoder.fit(\n",
    "    x_train_scaled, \n",
    "    x_train_scaled, \n",
    "    epochs=15, \n",
    "    batch_size=32, \n",
    "    validation_data=(x_test_scaled, x_test_scaled)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97f5f5048155191e25e7594c3d530b5ff4e9dff284a45544d3319aeed2095960"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
